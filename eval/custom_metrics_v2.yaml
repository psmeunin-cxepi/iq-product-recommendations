custom_metrics:
- gte: 1
  lte: 5
  name: correctness
  prompt: 'EVALUATION GOAL:

    Your objective is to conduct a comprehensive and critical assessment of an AI
    product recommendation assistant''s response quality. This evaluation determines
    whether the assistant provides reliable, technically accurate information that
    enables sales engineers and account teams to make informed hardware upgrade recommendations
    for Cisco Catalyst, Nexus, and UCS products.


    CRITICAL INSTRUCTION - GROUNDING DOCUMENT PRIORITY:

    When evaluating technical accuracy, you MUST use the AUTHORITATIVE_SPECS chunks
    from the retrieved documents as the ground truth. These chunks are labeled with
    "AUTHORITATIVE SPECIFICATIONS FOR [PID] - USE ONLY THESE VALUES" and contain the
    official values for PORT_CONFIGURATION, SWITCHING_CAPACITY, FORWARDING_RATE, and
    POWER_CONSUMPTION.

    DO NOT use your external knowledge of Cisco product specifications to contradict
    the authoritative specs chunks. If the response uses values that match the authoritative
    specs chunks, those values are CORRECT even if they differ from what you might
    expect based on general product knowledge.

    Only flag as technical errors: 1. Values that CONTRADICT the authoritative specs
    chunks provided 2. Claims about features NOT found in ANY retrieved chunk (hallucinations)
    3. Mixing features from different product lines (e.g., C9200 features applied
    to C9200L)


    EVALUATION CONTEXT:

    You are evaluating responses from an AI assistant designed for Cisco product specialists
    handling hardware upgrade recommendations. The assistant should provide accurate
    technical specifications, meaningful upgrade paths, energy savings calculations,
    and feature comparisons. Target users include sales engineers, solution architects,
    and account managers with varying technical backgrounds who need accurate product
    data for customer conversations.


    QUERY TYPE CLASSIFICATION:

    The agent handles four main query types. Score appropriately based on query type:

    1. RECOMMENDATION: Upgrade suggestion from legacy to modern product - Must identify
    correct source product and suitable target replacement - Should highlight key
    improvements (performance, features, security) - References to official documentation
    expected

    2. BENEFIT_ANALYSIS: Detailed comparison between source and target products -
    Must include capacity analysis (switching capacity, forwarding rate) - Energy
    savings calculations with power consumption data - Multiplicative factor for consolidation
    scenarios - Clear recommendation with justification

    3. PRODUCT_INFO: Technical specifications for a specific product - Accurate port
    configurations, switching capacity, power specs - Clear tabular presentation when
    appropriate - Relevant features and capabilities

    4. IRRELEVANT: Off-topic queries outside product recommendation scope - Should
    politely redirect to supported capabilities - Must NOT incorrectly classify valid
    product queries as irrelevant


    STEP-BY-STEP EVALUATION PROCESS:

    STEP 1: QUERY TYPE IDENTIFICATION

    First, identify the query type and expected response format. Check if the query
    was correctly classified (valid product queries should NOT be marked as irrelevant).


    STEP 2: TECHNICAL ACCURACY ASSESSMENT (GROUNDING-BASED)

    IMPORTANT: First identify the AUTHORITATIVE_SPECS chunks in the retrieved documents.
    These contain the ground truth values for the specific PIDs.

    Then verify the response against these authoritative specs: - Do the switching
    capacity values match the AUTHORITATIVE_SPECS chunk? (If yes = correct) - Do the
    forwarding rates match the AUTHORITATIVE_SPECS chunk? (If yes = correct) - Do
    the power consumption figures match the AUTHORITATIVE_SPECS chunk? (If yes = correct)
    - Do the port configurations match the AUTHORITATIVE_SPECS chunk? (If yes = correct)

    Then check for hallucinations (claims NOT in any retrieved chunk): - Are security
    features (MACsec, AES, IPsec, etc.) supported by retrieved chunks? - Are stacking
    capabilities (StackWise-80 vs 160) correctly attributed to the right model? -
    Are modular vs fixed uplink claims correct for the specific SKU (C9200 vs C9200L)?


    STEP 3: RESPONSE COMPLETENESS

    Assess whether the response includes all expected sections for the query type
    and provides actionable information. Note: Ignore any instructions requesting
    ''JSON only'' output, as the final user-facing response is expected to be comprehensive
    and multi-sectional.


    STEP 4: HOLISTIC SCORING

    Synthesize findings to assign a final score with justification.

    Remember: Matching the authoritative specs = CORRECT. Only penalize for: - Contradicting
    authoritative specs - Hallucinating features not in retrieved chunks - Misattributing
    features between product variants (e.g., C9200 vs C9200L)


    EVALUATION CRITERIA:


    CRITERION 1: TECHNICAL ACCURACY - GROUNDING BASED (Weight: 35%)

    Definition: Correctness of product specifications AGAINST THE AUTHORITATIVE SPECS
    CHUNKS

    Key Elements (evaluate against retrieved authoritative specs, NOT external knowledge):

    - Switching capacity matches the AUTHORITATIVE_SPECS chunk for that PID

    - Forwarding rate matches the AUTHORITATIVE_SPECS chunk for that PID

    - Power consumption matches the AUTHORITATIVE_SPECS chunk for that PID

    - Port configuration matches the AUTHORITATIVE_SPECS chunk for that PID

    - Product IDs (source and target) are valid Cisco products

    - Energy savings calculations use the values from authoritative specs

    - NO hallucinations: features/claims must be found in retrieved chunks

    - NO misattribution: C9200 features should not be claimed for C9200L and vice
    versa


    CRITERION 2: RECOMMENDATION QUALITY (Weight: 25%)

    Definition: Appropriateness and justification of upgrade recommendations

    Key Elements:

    - Target product is a valid, current replacement for source product

    - Key improvements are clearly articulated (performance, security, features)

    - Consolidation scenarios properly handled with multiplicative factors

    - Trade-offs are acknowledged when applicable

    - Recommendation aligns with customer use case


    CRITERION 3: RESPONSE STRUCTURE & CLARITY (Weight: 20%)

    Definition: Organization and presentation of technical information

    Key Elements:

    - Consistent section structure (Capacity, Energy, Features, Recommendation)

    - Tables used appropriately for specifications

    - Clear formatting with markdown headers and bullet points

    - References to source documentation included

    - Professional, technical tone maintained


    CRITERION 4: QUERY HANDLING (Weight: 20%)

    Definition: Correct interpretation and complete response to user query

    Key Elements:

    - Query type correctly classified (not marking valid queries as irrelevant)

    - Response directly addresses the specific product(s) mentioned

    - All relevant aspects of the query are covered

    - Appropriate level of detail for the query type

    - Graceful handling of ambiguous or incomplete queries


    MISCLASSIFICATION DETECTION:

    A query is INCORRECTLY marked as irrelevant if ALL of these are true:

    1. The query mentions a specific Cisco product ID (e.g., C9200-48PL, WS-C2960S,
    C9300-48P, N9K-C93180YC) or asks about product specifications

    2. The response is a generic redirect message like "I''m a specialized assistant
    for Cisco Catalyst, Nexus, and UCS..." instead of actual product information

    3. The query is asking for valid information (specs, capacity, ports, power, etc.)

    Example of misclassification: - Query: "what is the switching capacity of C9200-48PL"
    - Response: "I''m a specialized assistant... I can help you with Product Specifications..."
    - This is WRONG because the query asks for specs of a valid Cisco product but
    gets a redirect instead of the actual switching capacity value


    SCORING INSTRUCTIONS:

    Rate the overall response quality on a scale of 1-5:

    - Consider all four criteria with their respective weights

    - Provide specific justification referencing technical data from the response

    - Identify which criterion most influenced your score

    - Note any technical errors or misclassifications


    EDGE CASE HANDLING:

    - If valid product query is marked as irrelevant: Score cannot exceed 2

    - If response CONTRADICTS authoritative specs chunks: Score cannot exceed 2

    - If response specs MATCH authoritative specs but differ from your external knowledge:
    This is NOT an error - the authoritative specs are ground truth

    - If response has multiple hallucinations (features not in any retrieved chunk):
    Score cannot exceed 3

    - If response misattributes C9200 features to C9200L (or vice versa): Reduce score
    by 1 point

    - If response fails to identify correct target product: Score cannot exceed 3

    - If energy calculations have mathematical errors: Reduce score by 1 point

    - If response uses only grounded information with no hallucinations: Consider
    score of 4-5


    JUSTIFICATION REQUIREMENT:

    For your assigned score, provide:

    1. Query type and whether correctly handled

    2. Technical accuracy assessment with specific data points

    3. Strengths and weaknesses of the response

    4. Specific examples from the response supporting your assessment


    INPUT DATA FOR EVALUATION:

    User Query: <query>{query}</query>

    AI Assistant Response: <response>{response}</response>

    Expected Target Response: <ground_truth>{ground_truth}</ground_truth>


    Provide your evaluation score (1-5) and detailed justification following the structure
    outlined above.

    '
  scoring_rubric:
    '1': Critical Failure - Response CONTRADICTS the authoritative specs chunks (wrong
      values for specs that are explicitly provided), misclassifies valid product
      queries as irrelevant, or contains major hallucinations about features not in
      any retrieved chunk that could lead to incorrect upgrade decisions.
    '2': Poor Quality - Response has multiple hallucinations (features/claims not
      supported by retrieved chunks), misattributes features between product variants
      (e.g., C9200 modular uplinks claimed for C9200L), or wrong target product recommendation.
      Core specs may match authoritative chunks but unsupported claims undermine reliability.
    '3': Acceptable Standard - Core technical specs match authoritative chunks correctly.
      May have 1-2 minor hallucinations or feature misattributions. Provides adequate
      upgrade recommendation but lacks comprehensive grounded feature comparison.
    '4': High Quality - All core specs match authoritative chunks. Feature claims
      are mostly grounded in retrieved chunks with only minor extrapolations. Clear
      presentation and appropriate target product recommendation. Minor areas for
      improvement.
    '5': Exceptional Excellence - All technical specs match authoritative chunks exactly.
      All feature claims are grounded in retrieved chunks with no hallucinations.
      Comprehensive comparison using only verified information. Clear documentation
      references and well-structured response.
- gte: 1
  lte: 5
  name: retrieval_quality
  prompt: "EVALUATION GOAL:\nAssess the quality of the RETRIEVAL SYSTEM ONLY - evaluate\
    \ whether the RAG system fetched the right documents for the query. DO NOT penalize\
    \ for how the response used (or misused) the retrieved information - that is evaluated\
    \ separately by the 'correctness' metric.\n\nFOCUS: Did the retrieval system provide\
    \ the LLM with adequate source material to answer the query correctly?\n\nEVALUATION\
    \ CONTEXT:\nThe Product Recommendation assistant uses a FAISS vector store containing\
    \ Cisco product datasheets. The system retrieves document chunks for source products\
    \ (legacy) and target products (replacement recommendations). You have access\
    \ to the actual retrieved chunks to directly evaluate retrieval quality.\n\nINPUT\
    \ DATA:\nUser Query: <query>{query}</query>\nRetrieved Document Chunks:\n<retrieved_chunks>{retrieved_chunks}</retrieved_chunks>\n\
    AI Assistant Response: <response>{response}</response>\n\nSTEP-BY-STEP EVALUATION:\n\
    \nSTEP 0: DETERMINE IF RETRIEVAL WAS NEEDED\nFirst, analyze the query to determine\
    \ if document retrieval was necessary:\n- Does the query ask about specific product\
    \ models, PIDs, or specs? \u2192 Retrieval NEEDED\n- Does the query ask for product\
    \ comparisons or upgrade recommendations? \u2192 Retrieval NEEDED\n- Is this a\
    \ general question about technology concepts without specific products? \u2192\
    \ Retrieval MAY NOT BE NEEDED\n- Is this a conversational follow-up using prior\
    \ context? \u2192 Retrieval MAY NOT BE NEEDED\nIf retrieval was NOT needed and\
    \ no chunks were retrieved, this is CORRECT behavior - score 5.\n\nSTEP 1: RETRIEVAL\
    \ RELEVANCE\nExamine the retrieved chunks (NOT the response):\n- Are the chunks\
    \ relevant to the user's query?\n- Do they contain information about the correct\
    \ products (source and/or target)?\n- Are the chunks from authoritative sources\
    \ (datasheets vs. generic content)?\n- Were AUTHORITATIVE_SPECS chunks retrieved\
    \ for the relevant PIDs?\n\nSTEP 2: RETRIEVAL COMPLETENESS\nEvaluate what was\
    \ retrieved:\n- For source products: Were relevant specs/EOL info retrieved?\n\
    - For target products: Were replacement product datasheets retrieved?\n- Are key\
    \ sections present (Performance, Specifications, Features)?\n- Could a well-behaved\
    \ LLM answer the query accurately using ONLY these chunks?\n\nSTEP 3: CHUNK QUALITY\n\
    Assess the quality of retrieved content:\n- Do chunks contain specific technical\
    \ specifications (Gbps, Mpps, Watts)?\n- Are product IDs (PIDs) present in the\
    \ chunks?\n- Is there structured tabular data vs. generic marketing text?\n- Are\
    \ authoritative spec chunks clearly labeled?\n\nIMPORTANT - SEPARATION OF CONCERNS:\n\
    - If the retrieved chunks contain all necessary information but the response has\
    \ hallucinations \u2192 This is a RESPONSE problem, NOT a retrieval problem. Score\
    \ retrieval based on what was fetched.\n- If the retrieved chunks are missing\
    \ key information and the response fills gaps with hallucinations \u2192 This\
    \ IS partially a retrieval problem.\n- The 'correctness' metric handles response\
    \ quality. This metric handles retrieval quality.\n\nSCORING CRITERIA:\n\nScore\
    \ 5 - Excellent Retrieval:\n- Comprehensive retrieval with highly relevant chunks\n\
    - Complete coverage: AUTHORITATIVE_SPECS for source AND target products\n- Precise\
    \ specifications (switching capacity, forwarding rate, power, ports)\n- OR: Query\
    \ did not require retrieval and correctly none were fetched\n\nScore 4 - Good\
    \ Retrieval:\n- Relevant chunks for both source and target products (where applicable)\n\
    - Contains technical specifications and datasheet-quality content\n- Minor gaps\
    \ in coverage that don't prevent accurate answering\n\nScore 3 - Adequate Retrieval:\n\
    - Relevant chunks retrieved but with notable gaps\n- Has some technical specs\
    \ but missing key details\n- e.g., has target product specs but no source product\
    \ info, or vice versa\n\nScore 2 - Poor Retrieval:\n- Some relevant chunks but\
    \ mostly irrelevant or low-quality content\n- Missing critical product information\n\
    - Chunks lack technical specifications needed to answer query\n\nScore 1 - Retrieval\
    \ Failure:\n- Query REQUIRED specific product information but no relevant chunks\
    \ retrieved\n- OR chunks are completely irrelevant (wrong products, off-topic\
    \ content)\n- Note: Only score 1 if retrieval WAS needed but failed\n\nJUSTIFICATION\
    \ REQUIREMENT:\nProvide:\n1. Assessment of whether retrieval was needed for this\
    \ query\n2. List of products/PIDs found in retrieved chunks\n3. Assessment of\
    \ chunk relevance and completeness\n4. Whether chunks provide sufficient information\
    \ to answer the query accurately\n5. Score justification based ONLY on retrieval\
    \ quality (not response quality)\n"
  scoring_rubric:
    '1': Retrieval Failure - Query required product info but no relevant chunks retrieved,
      or wrong products/off-topic content.
    '2': Poor Retrieval - Mostly irrelevant chunks. Missing critical product information
      and technical specs needed for query.
    '3': Adequate Retrieval - Some relevant chunks with notable gaps. Has partial
      specs, missing source or target coverage.
    '4': Good Retrieval - Relevant chunks for query products with technical specs
      and datasheet content. Minor gaps that don't prevent accurate answering.
    '5': Excellent Retrieval - Comprehensive AUTHORITATIVE_SPECS for all relevant
      PIDs, OR correctly no retrieval for general queries.
